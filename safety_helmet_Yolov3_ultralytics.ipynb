{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불러오기"
      ],
      "metadata": {
        "id": "qYllijbbg_Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 캐글 설치\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "Hh__GD0xf03u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .kaggle 폴더 생성\n",
        "!mkdir -p ~/.kaggle/\n",
        "# kaggle.josn .kaggle로 복사\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# 복사 확인\n",
        "!ls ~/.kaggle"
      ],
      "metadata": {
        "id": "2dRxr5EAgIXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ~"
      ],
      "metadata": {
        "id": "tjve_H-XgOHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "WRyUo6U_E5u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 권한 변경 : 읽기, 쓰기, 실행 (rwx)\n",
        "!chmod 600 .kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hwrw4uwNgOdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading dataset\n",
        "!kaggle datasets download -d vodan37/yolo-helmethead"
      ],
      "metadata": {
        "id": "gqTPPiaJgP2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q yolo-helmethead.zip"
      ],
      "metadata": {
        "id": "QSFdVwXggTBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 불러오기"
      ],
      "metadata": {
        "id": "DCVvsQklF6Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov3  # 깃허브 모델을 불러온다 \n",
        "%cd yolov3\n",
        "!pip install -r requirements.txt  # 모델에 필요한 조건들을 설치 "
      ],
      "metadata": {
        "id": "mgsM0AuDBAYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터를 모델 양식에 맞춰주기"
      ],
      "metadata": {
        "id": "3ev13HwVuH41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습에 넣을 데이터 양식\n",
        "# 데이터에 맞게 모델에 넣을 양식을 작성해준다 \n",
        "# 실습에 사용한 Yolo v4는 다운받은 이미지와 라벨들을 모델안에 옮겨줘야 했으나\n",
        "# \n",
        "# path: ../helm/helm\n",
        "# train: images/train\n",
        "# val: images/valid \n",
        "# test: images/test\n",
        "# #classes \n",
        "# nc: 2 \n",
        "# names: ['head', 'helmet']\n",
        "\n",
        "# coco128.yaml을 보고 양식을 직접 작성해도 되지만 코드 한번 돌려서 넣어주는게 더 편하므로 이렇게 코드를 짜보았다 \n",
        "t = open('/content/yolov3/data/helm.yaml', 'w')\n",
        "t.write('path: ../helm/helm\\n')\n",
        "t.write('train: images/train\\n')\n",
        "t.write('val: images/valid\\n')\n",
        "t.write('test: images/test\\n')\n",
        "t.write('nc: 2\\n')\n",
        "t.write(\"names: ['head', 'helmet']\")\n",
        "t.close()"
      ],
      "metadata": {
        "id": "kyrKST79YFHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 훈련"
      ],
      "metadata": {
        "id": "PplX-7DyuCPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Yolo v3 모델로 우리 조의 헬멧데이터, 커스텀 데이터를 yolov3.pt, 이미 훈련된 가중치를 불러와서 전이학습을 시켰다 \n",
        "# 코드 내에 yolo3.pt를 다운 받는 링크가 있어서 실행시키면 자동으로 웨이트를 다운받는다 \n",
        "# 1epoch 당 30분씩 걸림 \n",
        "!python train.py --img 640 --batch 16 --epochs 15 --data helm.yaml --weights yolov3.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TRQZ57j_LaS",
        "outputId": "938fe65b-1a8b-457b-961e-dfdd0390d73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov3.pt, cfg=, data=helm.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov3 ✅\n",
            "YOLOv3 🚀 v9.6.0-16-gd58ba5e torch 1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv3 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3.pt to yolov3.pt...\n",
            "100% 119M/119M [00:14<00:00, 8.56MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     20672  models.common.Bottleneck                [64, 64]                      \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    164608  models.common.Bottleneck                [128, 128]                    \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  8   2627584  models.common.Bottleneck                [256, 256]                    \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  8  10498048  models.common.Bottleneck                [512, 512]                    \n",
            "  9                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            " 10                -1  4  20983808  models.common.Bottleneck                [1024, 1024]                  \n",
            " 11                -1  1   5245952  models.common.Bottleneck                [1024, 1024, False]           \n",
            " 12                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 13                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 14                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 15                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 16                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   1377792  models.common.Bottleneck                [768, 512, False]             \n",
            " 20                -1  1   1312256  models.common.Bottleneck                [512, 512, False]             \n",
            " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 22                -1  1   1180672  models.common.Conv                      [256, 512, 3, 1]              \n",
            " 23                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 24                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 25           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  1    344832  models.common.Bottleneck                [384, 256, False]             \n",
            " 27                -1  2    656896  models.common.Bottleneck                [256, 256, False]             \n",
            " 28      [27, 22, 15]  1     37695  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 333 layers, 61529119 parameters, 61529119 gradients, 154.9 GFLOPs\n",
            "\n",
            "Transferred 433/439 items from yolov3.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 72 weight, 75 weight (no decay), 75 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv3, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../helm/helm/labels/train' images and labels...15887 found, 0 missing, 0 empty, 0 corrupted: 100% 15887/15887 [00:02<00:00, 5472.80it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../helm/helm/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../helm/helm/labels/valid' images and labels...4641 found, 0 missing, 0 empty, 0 corrupted: 100% 4641/4641 [00:02<00:00, 2207.02it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../helm/helm/labels/valid.cache\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.84 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/4     11.3G    0.0823   0.06478   0.02013       255       640:  31% 309/993 [09:25<20:52,  1.83s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 625, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 522, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 330, in train\n",
            "    scaler.step(optimizer)  # optimizer.step\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\", line 338, in step\n",
            "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\", line 284, in _maybe_opt_step\n",
            "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\", line 284, in <genexpr>\n",
            "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "Mr58zeeIuQ3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference과정. 여기에 사용한 웨이트는 15epochs 학습한 가중치, best.pt를 사용했다. 원래는 run폴더안 train폴더에 exp1,2,3 등 순서대로 안에 저장이 되나 돌려서 가중치를 얻기엔 너무 오래 걸려서 \n",
        "# 전에 커스텀데이터로 넣어서 돌리고 다운받은 웨이트를 사용하였다 \n",
        "# detect.py를 작동하면 data의 images폴더안에 있는 이미지들을 detect를 실행하고 그 결과를 runs/detect/expn폴더에 저장한다 \n",
        "!python detect.py --weights best.pt --img 640 --conf 0.25 --source data/images\n",
        "!display.Image(filename='data/images/hardhat.jpg', width=600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sstrN7zwV-UX",
        "outputId": "614847fe-3754-4f70-9b35-6fd1fd161835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=data/images, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv3 🚀 v9.6.0-17-g3508a98 torch 1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61502815 parameters, 0 gradients, 154.7 GFLOPs\n",
            "image 1/4 /content/yolov3/data/images/bus.jpg: 640x480 3 heads, Done. (0.021s)\n",
            "image 2/4 /content/yolov3/data/images/hardhat.jpg: 480x640 3 helmets, Done. (0.024s)\n",
            "image 3/4 /content/yolov3/data/images/hardhatworker.jpg: 448x640 6 helmets, Done. (0.022s)\n",
            "image 4/4 /content/yolov3/data/images/zidane.jpg: 384x640 2 heads, Done. (0.020s)\n",
            "Speed: 0.3ms pre-process, 21.8ms inference, 1.2ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp3\u001b[0m\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `filename='data/images/hardhat.jpg','\n",
            "/bin/bash: -c: line 0: `display.Image(filename='data/images/hardhat.jpg', width=600)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련시켜서 얻은 best웨이트로 valid 데이터들에 대한 검증을 실행해보았다 \n",
        "# train과 detect와 마찬가지로 runs폴더에 val폴더가 생기면서 각종 평가지표와 적용 이미지들이 자동으로 저장된다  \n",
        "!python val.py --weights best.pt --data helm.yaml --img 640 --iou 0.65 --half"
      ],
      "metadata": {
        "id": "TE5JS4cSrXjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081f7ad2-6427-4cf5-fe90-698d88ea6403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov3/data/helm.yaml, weights=['best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv3 🚀 v9.6.0-17-g3508a98 torch 1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61502815 parameters, 0 gradients, 154.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../helm/helm/labels/valid' images and labels...4641 found, 0 missing, 0 empty, 0 corrupted: 100% 4641/4641 [00:01<00:00, 3969.49it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../helm/helm/labels/valid.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 146/146 [01:34<00:00,  1.55it/s]\n",
            "                 all       4641      38272      0.949      0.932      0.966      0.608\n",
            "                head       4641      25868      0.946      0.915      0.954      0.536\n",
            "              helmet       4641      12404      0.952      0.949      0.978      0.681\n",
            "Speed: 0.1ms pre-process, 12.5ms inference, 1.8ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run YOLOv3 on COCO test\n",
        "!python val.py --weights best.pt --data helm.yaml --img 640 --iou 0.65 --half --task test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx_QNARWj-bA",
        "outputId": "1b1e3a82-8486-40b7-b86e-efed9be0ff28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov3/data/helm.yaml, weights=['best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=test, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv3 🚀 v9.6.0-17-g3508a98 torch 1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61502815 parameters, 0 gradients, 154.7 GFLOPs\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning '../helm/helm/labels/test' images and labels...2261 found, 0 missing, 0 empty, 0 corrupted: 100% 2261/2261 [00:00<00:00, 3937.04it/s]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: ../helm/helm/labels/test.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 71/71 [00:49<00:00,  1.44it/s]\n",
            "                 all       2261      19968      0.949      0.898      0.944      0.585\n",
            "                head       2261      13217      0.943      0.905      0.947      0.529\n",
            "              helmet       2261       6751      0.955      0.892      0.941      0.642\n",
            "Speed: 0.1ms pre-process, 12.4ms inference, 1.9ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V_czhrdMuFAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}