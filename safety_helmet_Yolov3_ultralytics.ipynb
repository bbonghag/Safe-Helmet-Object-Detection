{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "qYllijbbg_Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ìºê¸€ ì„¤ì¹˜\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "Hh__GD0xf03u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .kaggle í´ë” ìƒì„±\n",
        "!mkdir -p ~/.kaggle/\n",
        "# kaggle.josn .kaggleë¡œ ë³µì‚¬\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# ë³µì‚¬ í™•ì¸\n",
        "!ls ~/.kaggle"
      ],
      "metadata": {
        "id": "2dRxr5EAgIXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ~"
      ],
      "metadata": {
        "id": "tjve_H-XgOHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "WRyUo6U_E5u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# íŒŒì¼ ê¶Œí•œ ë³€ê²½ : ì½ê¸°, ì“°ê¸°, ì‹¤í–‰ (rwx)\n",
        "!chmod 600 .kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hwrw4uwNgOdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading dataset\n",
        "!kaggle datasets download -d vodan37/yolo-helmethead"
      ],
      "metadata": {
        "id": "gqTPPiaJgP2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q yolo-helmethead.zip"
      ],
      "metadata": {
        "id": "QSFdVwXggTBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "DCVvsQklF6Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov3  # ê¹ƒí—ˆë¸Œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ë‹¤ \n",
        "%cd yolov3\n",
        "!pip install -r requirements.txt  # ëª¨ë¸ì— í•„ìš”í•œ ì¡°ê±´ë“¤ì„ ì„¤ì¹˜ "
      ],
      "metadata": {
        "id": "mgsM0AuDBAYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°ì´í„°ë¥¼ ëª¨ë¸ ì–‘ì‹ì— ë§ì¶°ì£¼ê¸°"
      ],
      "metadata": {
        "id": "3ev13HwVuH41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµì— ë„£ì„ ë°ì´í„° ì–‘ì‹\n",
        "# ë°ì´í„°ì— ë§ê²Œ ëª¨ë¸ì— ë„£ì„ ì–‘ì‹ì„ ì‘ì„±í•´ì¤€ë‹¤ \n",
        "# ì‹¤ìŠµì— ì‚¬ìš©í•œ Yolo v4ëŠ” ë‹¤ìš´ë°›ì€ ì´ë¯¸ì§€ì™€ ë¼ë²¨ë“¤ì„ ëª¨ë¸ì•ˆì— ì˜®ê²¨ì¤˜ì•¼ í–ˆìœ¼ë‚˜\n",
        "# \n",
        "# path: ../helm/helm\n",
        "# train: images/train\n",
        "# val: images/valid \n",
        "# test: images/test\n",
        "# #classes \n",
        "# nc: 2 \n",
        "# names: ['head', 'helmet']\n",
        "\n",
        "# coco128.yamlì„ ë³´ê³  ì–‘ì‹ì„ ì§ì ‘ ì‘ì„±í•´ë„ ë˜ì§€ë§Œ ì½”ë“œ í•œë²ˆ ëŒë ¤ì„œ ë„£ì–´ì£¼ëŠ”ê²Œ ë” í¸í•˜ë¯€ë¡œ ì´ë ‡ê²Œ ì½”ë“œë¥¼ ì§œë³´ì•˜ë‹¤ \n",
        "t = open('/content/yolov3/data/helm.yaml', 'w')\n",
        "t.write('path: ../helm/helm\\n')\n",
        "t.write('train: images/train\\n')\n",
        "t.write('val: images/valid\\n')\n",
        "t.write('test: images/test\\n')\n",
        "t.write('nc: 2\\n')\n",
        "t.write(\"names: ['head', 'helmet']\")\n",
        "t.close()"
      ],
      "metadata": {
        "id": "kyrKST79YFHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ í›ˆë ¨"
      ],
      "metadata": {
        "id": "PplX-7DyuCPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Yolo v3 ëª¨ë¸ë¡œ ìš°ë¦¬ ì¡°ì˜ í—¬ë©§ë°ì´í„°, ì»¤ìŠ¤í…€ ë°ì´í„°ë¥¼ yolov3.pt, ì´ë¯¸ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì „ì´í•™ìŠµì„ ì‹œì¼°ë‹¤ \n",
        "# ì½”ë“œ ë‚´ì— yolo3.ptë¥¼ ë‹¤ìš´ ë°›ëŠ” ë§í¬ê°€ ìˆì–´ì„œ ì‹¤í–‰ì‹œí‚¤ë©´ ìë™ìœ¼ë¡œ ì›¨ì´íŠ¸ë¥¼ ë‹¤ìš´ë°›ëŠ”ë‹¤ \n",
        "# 1epoch ë‹¹ 30ë¶„ì”© ê±¸ë¦¼ \n",
        "!python train.py --img 640 --batch 16 --epochs 15 --data helm.yaml --weights yolov3.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TRQZ57j_LaS",
        "outputId": "938fe65b-1a8b-457b-961e-dfdd0390d73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov3.pt, cfg=, data=helm.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov3 âœ…\n",
            "YOLOv3 ğŸš€ v9.6.0-16-gd58ba5e torch 1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv3 ğŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3.pt to yolov3.pt...\n",
            "100% 119M/119M [00:14<00:00, 8.56MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     20672  models.common.Bottleneck                [64, 64]                      \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    164608  models.common.Bottleneck                [128, 128]                    \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  8   2627584  models.common.Bottleneck                [256, 256]                    \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  8  10498048  models.common.Bottleneck                [512, 512]                    \n",
            "  9                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            " 10                -1  4  20983808  models.common.Bottleneck                [1024, 1024]                  \n",
            " 11                -1  1   5245952  models.common.Bottleneck                [1024, 1024, False]           \n",
            " 12                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 13                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 14                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 15                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 16                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   1377792  models.common.Bottleneck                [768, 512, False]             \n",
            " 20                -1  1   1312256  models.common.Bottleneck                [512, 512, False]             \n",
            " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 22                -1  1   1180672  models.common.Conv                      [256, 512, 3, 1]              \n",
            " 23                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 24                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 25           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  1    344832  models.common.Bottleneck                [384, 256, False]             \n",
            " 27                -1  2    656896  models.common.Bottleneck                [256, 256, False]             \n",
            " 28      [27, 22, 15]  1     37695  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 333 layers, 61529119 parameters, 61529119 gradients, 154.9 GFLOPs\n",
            "\n",
            "Transferred 433/439 items from yolov3.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 72 weight, 75 weight (no decay), 75 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv3, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../helm/helm/labels/train' images and labels...15887 found, 0 missing, 0 empty, 0 corrupted: 100% 15887/15887 [00:02<00:00, 5472.80it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../helm/helm/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../helm/helm/labels/valid' images and labels...4641 found, 0 missing, 0 empty, 0 corrupted: 100% 4641/4641 [00:02<00:00, 2207.02it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../helm/helm/labels/valid.cache\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.84 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/4     11.3G    0.0823   0.06478   0.02013       255       640:  31% 309/993 [09:25<20:52,  1.83s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 625, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 522, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 330, in train\n",
            "    scaler.step(optimizer)  # optimizer.step\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\", line 338, in step\n",
            "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\", line 284, in _maybe_opt_step\n",
            "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\", line 284, in <genexpr>\n",
            "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "Mr58zeeIuQ3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferenceê³¼ì •. ì—¬ê¸°ì— ì‚¬ìš©í•œ ì›¨ì´íŠ¸ëŠ” 15epochs í•™ìŠµí•œ ê°€ì¤‘ì¹˜, best.ptë¥¼ ì‚¬ìš©í–ˆë‹¤. ì›ë˜ëŠ” runí´ë”ì•ˆ trainí´ë”ì— exp1,2,3 ë“± ìˆœì„œëŒ€ë¡œ ì•ˆì— ì €ì¥ì´ ë˜ë‚˜ ëŒë ¤ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì–»ê¸°ì—” ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ì„œ \n",
        "# ì „ì— ì»¤ìŠ¤í…€ë°ì´í„°ë¡œ ë„£ì–´ì„œ ëŒë¦¬ê³  ë‹¤ìš´ë°›ì€ ì›¨ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤ \n",
        "# detect.pyë¥¼ ì‘ë™í•˜ë©´ dataì˜ imagesí´ë”ì•ˆì— ìˆëŠ” ì´ë¯¸ì§€ë“¤ì„ detectë¥¼ ì‹¤í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ runs/detect/expní´ë”ì— ì €ì¥í•œë‹¤ \n",
        "!python detect.py --weights best.pt --img 640 --conf 0.25 --source data/images\n",
        "!display.Image(filename='data/images/hardhat.jpg', width=600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sstrN7zwV-UX",
        "outputId": "614847fe-3754-4f70-9b35-6fd1fd161835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=data/images, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv3 ğŸš€ v9.6.0-17-g3508a98 torch 1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61502815 parameters, 0 gradients, 154.7 GFLOPs\n",
            "image 1/4 /content/yolov3/data/images/bus.jpg: 640x480 3 heads, Done. (0.021s)\n",
            "image 2/4 /content/yolov3/data/images/hardhat.jpg: 480x640 3 helmets, Done. (0.024s)\n",
            "image 3/4 /content/yolov3/data/images/hardhatworker.jpg: 448x640 6 helmets, Done. (0.022s)\n",
            "image 4/4 /content/yolov3/data/images/zidane.jpg: 384x640 2 heads, Done. (0.020s)\n",
            "Speed: 0.3ms pre-process, 21.8ms inference, 1.2ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp3\u001b[0m\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `filename='data/images/hardhat.jpg','\n",
            "/bin/bash: -c: line 0: `display.Image(filename='data/images/hardhat.jpg', width=600)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í›ˆë ¨ì‹œì¼œì„œ ì–»ì€ bestì›¨ì´íŠ¸ë¡œ valid ë°ì´í„°ë“¤ì— ëŒ€í•œ ê²€ì¦ì„ ì‹¤í–‰í•´ë³´ì•˜ë‹¤ \n",
        "# trainê³¼ detectì™€ ë§ˆì°¬ê°€ì§€ë¡œ runsí´ë”ì— valí´ë”ê°€ ìƒê¸°ë©´ì„œ ê°ì¢… í‰ê°€ì§€í‘œì™€ ì ìš© ì´ë¯¸ì§€ë“¤ì´ ìë™ìœ¼ë¡œ ì €ì¥ëœë‹¤  \n",
        "!python val.py --weights best.pt --data helm.yaml --img 640 --iou 0.65 --half"
      ],
      "metadata": {
        "id": "TE5JS4cSrXjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081f7ad2-6427-4cf5-fe90-698d88ea6403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov3/data/helm.yaml, weights=['best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv3 ğŸš€ v9.6.0-17-g3508a98 torch 1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61502815 parameters, 0 gradients, 154.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../helm/helm/labels/valid' images and labels...4641 found, 0 missing, 0 empty, 0 corrupted: 100% 4641/4641 [00:01<00:00, 3969.49it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../helm/helm/labels/valid.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 146/146 [01:34<00:00,  1.55it/s]\n",
            "                 all       4641      38272      0.949      0.932      0.966      0.608\n",
            "                head       4641      25868      0.946      0.915      0.954      0.536\n",
            "              helmet       4641      12404      0.952      0.949      0.978      0.681\n",
            "Speed: 0.1ms pre-process, 12.5ms inference, 1.8ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run YOLOv3 on COCO test\n",
        "!python val.py --weights best.pt --data helm.yaml --img 640 --iou 0.65 --half --task test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx_QNARWj-bA",
        "outputId": "1b1e3a82-8486-40b7-b86e-efed9be0ff28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov3/data/helm.yaml, weights=['best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=test, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv3 ğŸš€ v9.6.0-17-g3508a98 torch 1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61502815 parameters, 0 gradients, 154.7 GFLOPs\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning '../helm/helm/labels/test' images and labels...2261 found, 0 missing, 0 empty, 0 corrupted: 100% 2261/2261 [00:00<00:00, 3937.04it/s]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: ../helm/helm/labels/test.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 71/71 [00:49<00:00,  1.44it/s]\n",
            "                 all       2261      19968      0.949      0.898      0.944      0.585\n",
            "                head       2261      13217      0.943      0.905      0.947      0.529\n",
            "              helmet       2261       6751      0.955      0.892      0.941      0.642\n",
            "Speed: 0.1ms pre-process, 12.4ms inference, 1.9ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V_czhrdMuFAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}